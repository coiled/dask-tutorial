{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "036478b0-c89d-4051-a513-f4436f48968f",
   "metadata": {},
   "source": [
    "## Get better at dask dataframes\n",
    "\n",
    "In this lesson you will learn some good practices for dask dataframes and dealing with data in general.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4307fa56-f051-467b-bf1f-e3acae08e3a8",
   "metadata": {},
   "source": [
    "### Work close to your data\n",
    "\n",
    "To get started when you are working with data that is in the cloud it's always better to work close to your data, to minimize the impact of IO networking. \n",
    "\n",
    "In this lesson, we will use coiled clusters that will be created on the same region that our datasets are stored. (the region is `\"us-east-2\"`)\n",
    "\n",
    "**NOTE:**\n",
    "If you do not have access to a coiled cluster you, can follow along just make sure you use the smaller dataset (use the `\"0.5GB-\"` ones). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a52c65b-f0bc-4341-9841-4de07f710dce",
   "metadata": {},
   "source": [
    "## Parquet vs CSV\n",
    "\n",
    "Most people are familiarized with csv files, but when it comes to working with data, working with parquet can make a big difference. The Parquet file format is column-oriented and it's designed to efficiently store and retrieve data. \n",
    "\n",
    "**Extra reading**\n",
    "You can read of the multiple advantages of using parquet data format in the blog [Advantages of Parquet File Format](https://www.coiled.io/blog/parquet-file-column-pruning-predicate-pushdown).\n",
    "\n",
    "Let's see an example where we compare reading the same data but in one case it is stored as `csv` files, while the other as `parquet` files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2104467-31be-4f61-ad22-fc9ceea8cb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "data ={\"0.5GB-csv\": \"s3://coiled-datasets/h2o-benchmark/N_1e7_K_1e2/*.csv\",\n",
    "       \"0.5GB-pq\": \"s3://coiled-datasets/h2o-benchmark/N_1e7_K_1e2_parquet/*.parquet\",\n",
    "       \"5GB-csv\": \"s3://coiled-datasets/h2o-benchmark/N_1e8_K_1e2/*.csv\",\n",
    "       \"5GB-pq\": \"s3://coiled-datasets/h2o-benchmark/N_1e8_K_1e2_parquet/*.parquet\",}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea1905f-09aa-4865-844e-d752639e29c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import coiled\n",
    "from dask.distributed import Client\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cf0ad8-5783-4ac4-80fa-1edc00a9eac0",
   "metadata": {},
   "source": [
    "### SECTION ON HOW TO LOGIN INTO COILED WHEN WE HAVE INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbdd9c6-dc76-48e4-9f89-34a35092ee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster = coiled.Cluster(name=\"dask-tutorial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ac86a4-5336-4c28-98e9-64323e450863",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cluster = coiled.Cluster(name=\"dask-tutorial\",\n",
    "                        n_workers=8,\n",
    "                        package_sync=True,\n",
    "                        backend_options={\"region_name\": \"us-east-2\"},\n",
    "                        );\n",
    "\n",
    "## maybe use mi6 instead, the default ones are slower..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254390a1-e055-446c-ab44-c8d76121928d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6829081-0fe8-432b-a126-254ba8139f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_csv = dd.read_csv(data[\"5GB-csv\"], storage_options={\"anon\": True})\n",
    "ddf_pq = dd.read_parquet(data[\"0.5GB-pq\"], storage_options={\"anon\": True})\n",
    "#dd.read_parquet(data[\"5GB-pq\"], storage_options={\"anon\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f938ab3c-30da-416c-87a4-58128c7973b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ddf_csv.groupby(\"id1\").agg({\"v1\": \"sum\"}).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a6f2fb-b1ba-4b8d-ade5-be1c2c6f4a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ddf_pq.groupby(\"id1\").agg({\"v1\": \"sum\"}).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0530b707-cad2-4b43-8921-1eb821616822",
   "metadata": {},
   "source": [
    "Notice that the `parquet` version without doing much it is already ~5X faster. \n",
    "\n",
    "Let's take a look at the dtypes in both cases and see if we can make some things faster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be1cc4f-4656-4565-bd83-dfec45e613fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca04e3e-036b-4990-bec4-136b0676bc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "##IF I SPECIFY THE DTYPES THIS GETS MUCH SLOWER ??? Thoughts??\n",
    "\n",
    "# ddf_csv = dd.read_csv(\n",
    "#             data[\"5GB-csv\"],\n",
    "#             dtype={\n",
    "#                 \"id1\": \"category\",\n",
    "#                 \"id2\": \"category\",\n",
    "#                 \"id3\": \"category\",\n",
    "#                 \"id4\": \"Int32\",\n",
    "#                 \"id5\": \"Int32\",\n",
    "#                 \"id6\": \"Int32\",\n",
    "#                 \"v1\": \"Int32\",\n",
    "#                 \"v2\": \"Int32\",\n",
    "#                 \"v3\": \"float64\",\n",
    "#             },\n",
    "#             storage_options={\"anon\": True},)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011266f8-b9d7-46c4-b88d-574582f5528b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ad40aa-c61b-4d3a-81c0-3c213cb7c688",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16240559-cff4-4694-8724-41976b6b8005",
   "metadata": {},
   "outputs": [],
   "source": [
    "## example to exaplain column prunning. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9fea26-1569-41e1-8cf4-c087995dc0e4",
   "metadata": {},
   "source": [
    "### Read about why in read_parquet we read the dtypes but not csv?\n",
    "\n",
    "- show ddf.partitions[0].memory_usage(deep=True).compute() / 1e6\n",
    "- see what happens with csv and with parquet, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afb1240-7afe-4f7e-9637-183dc8c0a125",
   "metadata": {},
   "source": [
    "## dtypes\n",
    "\n",
    "NOTE: \n",
    "\n",
    "FOR THE PUSPOSE OF THE TUTORIAL I NEED TO GENERATE THE DATA FOR 5GB WITH PYARROW STRINGS. \n",
    "OR TYPECAST, EXPLORE THAT.\n",
    "\n",
    "THEN RUN \n",
    "```python\n",
    "        ddf_q3 = ddf[[\"id3\", \"v1\", \"v3\"]].astype({\"id3\": \"string[pyarrow]\"})\n",
    "        (\n",
    "            ddf_q3.groupby(\"id3\", dropna=False, observed=True)\n",
    "            .agg({\"v1\": \"sum\", \"v3\": \"mean\"})  \n",
    "            .compute()\n",
    "        )\n",
    "```\n",
    "\n",
    "chat with james to see if there is anything else about pyarrow dtypes we could be showing here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744bb47c-bf29-4274-9490-3c8155330798",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d0abf57-1418-4262-ac9d-2ff5b1563b3c",
   "metadata": {},
   "source": [
    "## High cardinality \n",
    "\n",
    "- id1 has 100 unique values\n",
    "- id3 has 1_000_000 unique values\n",
    "\n",
    "Let's see what happens when we try to groupby on a high cardinality column, and what can we do to make this better. \n",
    "\n",
    "Read docs about shuffle, and explain advantages, extract useful info. Ask about p2p docs?\n",
    "https://docs.dask.org/en/stable/dataframe-groupby.html#shuffle-methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df01ba16-6e57-4dd9-b5eb-92172e4112da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd1409b-a01f-4894-b823-e299359c32c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#With 5 workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17d3b80-9b7c-4b45-ad00-4dc2f66c8ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU times: user 833 ms, sys: 338 ms, total: 1.17 s\n",
    "# Wall time: 3min 9s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8345920-2acf-466d-9e30-bde16b8809a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ddf = ddf_pq[[\"id3\", \"v1\", \"v3\"]]\n",
    "(\n",
    "    ddf.groupby(\"id3\")\n",
    "    .agg({\"v1\": \"sum\", \"v3\": \"mean\"})\n",
    "    .compute()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6e9aa2-e177-423d-b060-8affb15256e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using shuffle tasks is slower :/ explanation?\n",
    "##CPU times: user 1.58 s, sys: 858 ms, total: 2.44 s\n",
    "#Wall time: 4min 49s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4169829-02e0-4eea-bbff-804ce0da2275",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ddf = ddf_pq[[\"id3\", \"v1\", \"v3\"]]\n",
    "(\n",
    "    ddf.groupby(\"id3\")\n",
    "    .agg({\"v1\": \"sum\", \"v3\": \"mean\"}, shuffle=\"tasks\")\n",
    "    .compute()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7909ace5-72bb-4a04-a295-6b33eaf5ce77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THERE IS BUG, AND I CAN'T RUN THIS\n",
    "# SEE https://github.com/dask/dask/issues/9754\n",
    "%%time\n",
    "ddf = ddf_pq[[\"id3\", \"v1\", \"v3\"]]\n",
    "(\n",
    "    ddf.groupby(\"id3\")\n",
    "    .agg({\"v1\": \"sum\", \"v3\": \"mean\"}, shuffle=\"p2p\")\n",
    "    .compute()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cab0d41-2236-4d89-982f-222439d43209",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
