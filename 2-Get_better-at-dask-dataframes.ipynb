{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "036478b0-c89d-4051-a513-f4436f48968f",
   "metadata": {},
   "source": [
    "## Get better at dask dataframes\n",
    "\n",
    "In this lesson you will learn some good practices for dask dataframes and dealing with data in general.\n",
    "\n",
    "## Parquet is where is at!!\n",
    "\n",
    "You will learn the advantages of working with the parquet data format, and using the Uber/Lyft dataset you will learn to troubleshoot the nuances of working with real data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4307fa56-f051-467b-bf1f-e3acae08e3a8",
   "metadata": {},
   "source": [
    "### Work close to your data\n",
    "\n",
    "To get started when you are working with data that is in the cloud it's always better to work close to your data, to minimize the impact of IO networking. \n",
    "\n",
    "In this lesson, we will use coiled clusters that will be created on the same region that our datasets are stored. (the region is `\"us-east-2\"`)\n",
    "\n",
    "**NOTE:**\n",
    "If you do not have access to a coiled cluster you, can follow along just make sure you use the smaller dataset (use the `\"0.5GB-\"` ones). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a52c65b-f0bc-4341-9841-4de07f710dce",
   "metadata": {},
   "source": [
    "## Parquet vs CSV\n",
    "\n",
    "Most people are familiarized with csv files, but when it comes to working with data, working with parquet can make a big difference. The Parquet file format is column-oriented and it's designed to efficiently store and retrieve data. \n",
    "\n",
    "### Small motivation example: \n",
    "Let's see an example where we compare reading the same data but in one case it is stored as `csv` files, while the other as `parquet` files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2104467-31be-4f61-ad22-fc9ceea8cb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "data ={\"0.5GB-csv\": \"s3://coiled-datasets/h2o-benchmark/N_1e7_K_1e2/*.csv\",\n",
    "       \"0.5GB-pq\": \"s3://coiled-datasets/h2o-benchmark/N_1e7_K_1e2_parquet/*.parquet\",\n",
    "       \"5GB-csv\": \"s3://coiled-datasets/h2o-benchmark/N_1e8_K_1e2/*.csv\",\n",
    "       \"5GB-pq\": \"s3://coiled-datasets/h2o-benchmark/N_1e8_K_1e2_parquet/*.parquet\",}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea1905f-09aa-4865-844e-d752639e29c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import coiled\n",
    "from dask.distributed import Client\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ac86a4-5336-4c28-98e9-64323e450863",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cluster = coiled.Cluster(name=\"dask-tutorial\", #RETHINK NAME ADD RANDOM UUID\n",
    "                        n_workers=10,\n",
    "                        package_sync=True,\n",
    "                        backend_options={\"region_name\": \"us-east-2\"},\n",
    "                        );\n",
    "\n",
    "## maybe use mi6 instead, the default ones are slower..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254390a1-e055-446c-ab44-c8d76121928d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6829081-0fe8-432b-a126-254ba8139f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_csv = dd.read_csv(data[\"5GB-csv\"], storage_options={\"anon\": True})\n",
    "ddf_pq = dd.read_parquet(data[\"5GB-pq\"], storage_options={\"anon\": True})\n",
    "#dd.read_parquet(data[\"5GB-pq\"], storage_options={\"anon\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78de69de-fe56-46e4-b8b7-63936c7de7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a9742b-a915-4e7f-8d81-59a127bf9e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f938ab3c-30da-416c-87a4-58128c7973b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ddf_csv.groupby(\"id1\").agg({\"v1\": \"sum\"}).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a6f2fb-b1ba-4b8d-ade5-be1c2c6f4a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ddf_pq.groupby(\"id1\").agg({\"v1\": \"sum\"}).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0530b707-cad2-4b43-8921-1eb821616822",
   "metadata": {},
   "source": [
    "Notice that the `parquet` version without doing much it is already ~5X faster. \n",
    "\n",
    "Let's take a look at the memory usage as well as the `dtypes` in both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be1cc4f-4656-4565-bd83-dfec45e613fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## memory usage for 1 partition\n",
    "ddf_csv.partitions[0].memory_usage(deep=True).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011266f8-b9d7-46c4-b88d-574582f5528b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_pq.partitions[0].memory_usage(deep=True).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5aa3b67-328d-409a-8b9a-8930ca0da53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e323f48-9bbb-46bd-af08-7d6ded1ccead",
   "metadata": {},
   "source": [
    "### Uber/Lyft data transformation\n",
    "\n",
    "In the example above we quickly saw that the format in which the data is saved already makes a big difference. But there so much to exploit about the parquet file format. \n",
    "\n",
    "Let's work with the data from [High-Volume For-Hire Services](https://www.nyc.gov/site/tlc/businesses/high-volume-for-hire-services.page)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda99d96-4b61-4bd8-882b-d7b457279276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "\n",
    "s3 = s3fs.S3FileSystem()\n",
    "files = s3.glob(\"nyc-tlc/trip data/fhvhv_tripdata_*.parquet\")\n",
    "files[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af054b4-894c-4ff0-b5f2-e1dd9d117066",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64ee00a-f2e8-4aee-b587-0b807801d78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#not sure where the data is but I will write to a bucket in us-east-2\n",
    "cluster = coiled.Cluster(\n",
    "    n_workers=10,\n",
    "    name=\"nyc-uber-lyft\",\n",
    "    package_sync=True,\n",
    "    backend_options={\"region\": \"us-east-2\"}, \n",
    "    worker_memory=\"64 GiB\", #we know we need a lot of memory from experience\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183c584b-bf15-4808-9dcb-a76a105e9c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b48f56f-2937-4b6d-8477-7c07721981c9",
   "metadata": {},
   "source": [
    "## Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56cecd1-7af5-4f6a-bd9f-7cb188e09030",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419eefb7-767e-402f-b8c6-fa80b8aa1c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882652d4-cc6b-454a-be90-d6b8111186ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.read_parquet(\n",
    "    \"s3://nyc-tlc/trip data/fhvhv_tripdata_*.parquet\",\n",
    ")\n",
    "ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ea4e9a-d812-4d50-9f73-c34054b34007",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspect memory usage of 1 partition\n",
    "ddf.partitions[0].memory_usage(deep=True).compute().apply(dask.utils.format_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3727d3f4-3544-4889-8499-d4ea867bbf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspect dtypes\n",
    "ddf.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0547a9bf-436a-4a49-8cdd-7d79f4c281f0",
   "metadata": {},
   "source": [
    "## Challenges\n",
    "\n",
    "As you can see, the partitions are very big, and the data types are inefficient.\n",
    "\n",
    "## Recommendations and best practices:\n",
    "**Partition size**\n",
    "\n",
    "In general we aim for ~100MB (in memory) per partition. \n",
    "\n",
    "**dtypes**\n",
    "\n",
    "- Avoid object types for strings: use `\"string[pyarrow]\"`\n",
    "- Reduce int/float representation if possible\n",
    "- Use categorical dtypes when possible.\n",
    "\n",
    "### Create conversions dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac85d0af-b961-4fff-be71-b14b60cd52bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bccadb-6f7a-4dba-a4fd-113bd96fbbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversions = {}\n",
    "for column, dtype in ddf.dtypes.items():\n",
    "    if dtype == \"object\":\n",
    "        conversions[column] = \"string[pyarrow]\"\n",
    "    if dtype == \"float64\":\n",
    "        conversions[column] = \"float32\"\n",
    "    if dtype == \"int64\": \n",
    "        conversions[column] = \"int32\"\n",
    "    if \"flag\" in column:\n",
    "        conversions[column] = pd.CategoricalDtype(categories=[\"Y\", \"N\"])\n",
    "    if column == \"airport_fee\":\n",
    "        conversions[column] = \"float32\"  #noticed that this has floats and the <NA> is making it an object\n",
    "conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53645d0d-8cd9-4fe8-892a-b4c11f2b787d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = ddf.astype(conversions)\n",
    "ddf = ddf.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed8a8ed-abb8-4fa4-8686-3a7a68bd28df",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.partitions[0].memory_usage(deep=True).compute().apply(dask.utils.format_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5970a21-6c8c-40dd-869e-07d998e7c2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.utils.format_bytes(\n",
    "    ddf.partitions[0].memory_usage(deep=True).compute().sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec42ac57-72fd-4ff3-8416-dcdb287e25b7",
   "metadata": {},
   "source": [
    "### Repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4815f73c-0ae4-4b8b-84f2-1750cbaa969a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = ddf.repartition(partition_size=\"128MB\").persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c13761-4906-47a6-b138-1ec5875e1242",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.utils.format_bytes(\n",
    "    ddf.memory_usage(deep=True).compute().sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329c807b-1dfe-4173-9d34-73bc8fc38695",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.npartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d214655b-2850-49bd-8e6b-70f51ef761b7",
   "metadata": {},
   "source": [
    "## Sort and one-day partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b208ec5-4c1e-45e7-921e-04a90b60f37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = ddf.set_index(\"request_datetime\").persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b056e6e8-f393-4bae-906a-bdc411c9afb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.divisions[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf67360c-83b8-4dbb-ac2c-340a51e329e6",
   "metadata": {},
   "source": [
    "Look like they are a bit longer than a day, we might as well repartition them witha  1-day frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c66047-19cc-4348-a58f-2a7e1b10330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = ddf.repartition(freq=\"1d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c174e7-9022-4afc-8fd4-571d17245c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.divisions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ca0e10-fc36-476f-8721-54efb6c51370",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.npartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de701dd-2f55-418f-908a-3ae8839f2352",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clever name for files when to_parquet\n",
    "divisions = ddf.divisions\n",
    "\n",
    "def name_file(index: int) -> str:\n",
    "    return str(divisions[index].date()) + \".parquet\"\n",
    "\n",
    "name_file(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0d899c-2155-4872-ade7-d80a19f5190e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.to_parquet(\n",
    "    \"s3://coiled-datasets/uber-lyft-tlc/\", \n",
    "    name_function=name_file,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287e76d2-04ce-4f5c-a620-39017f8b05a0",
   "metadata": {},
   "source": [
    "## Read data back\n",
    "\n",
    "use_nullable_dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab0e94b-0725-40b2-bc04-52773608edf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a340ae-4f3b-4714-bce4-2534a65c3b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_parquet(\n",
    "    \"s3://coiled-datasets/uber-lyft-tlc/\", \n",
    "    use_nullable_dtypes=True\n",
    ").astype({\"hvfhs_license_num\": \"string[pyarrow]\", \n",
    "         \"dispatching_base_num\": \"string[pyarrow]\",\n",
    "         \"originating_base_num\": \"string[pyarrow]\",\n",
    "         }).persist()\n",
    "#df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a28eda-ee5f-452f-9941-6bbbc8f9aa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0001de44-e6fd-4f45-aa3b-3d51531e48d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hvfhs_license_num.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1264d50b-8d5f-461c-9927-de62ef16978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.utils.format_bytes(\n",
    "    df.memory_usage(deep=True).sum().compute()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11baab3-052b-43d5-bf46-ece1e020ab9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Note:\n",
    "\n",
    "Without pyarrow strings we get '~200GB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4291e8-0dce-467b-95ce-badd2513b10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beeff184-6c3d-44bb-86f3-f071c1ab5781",
   "metadata": {},
   "source": [
    "# On to a smaller cluster - let's do data analysis\n",
    "\n",
    "Now we are at a stage that our whole dataset is ~75GB in memory. This is something we can work with in a smaller cluster. But also, when it comes to exploring data we do not necessarily need the whole data set.\n",
    "\n",
    "One of the beauties of the parquet file format are:\n",
    "\n",
    "- Column pruning: Get only the data of the column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3476887b-5643-4e4a-9cdb-d2488692cdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = coiled.Cluster(name=\"uber-lyft-small\",\n",
    "                         n_workers=10, \n",
    "                         package_sync=True,\n",
    "                         backend_options={\"region_name\": \"us-east-2\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304ba45b-5604-4bdd-8a33-7433cd9df26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a715c5-b809-4305-93d1-466a1008326c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15530dc1-703e-4491-9eec-c391cd663a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_parquet(\n",
    "    \"s3://coiled-datasets/uber-lyft-tlc/\", \n",
    "    use_nullable_dtypes=True\n",
    ").astype({\"hvfhs_license_num\": \"string[pyarrow]\", \n",
    "         \"dispatching_base_num\": \"string[pyarrow]\",\n",
    "         \"originating_base_num\": \"string[pyarrow]\",\n",
    "         })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff7b58b-97c5-4e06-9fdf-f2fe4fc979a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cdbc9a-3451-487d-80c5-45c4b1c2cd9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f52fc7-259f-46f4-935f-fe8d5160bc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small = df[[\"base_passenger_fare\", \"driver_pay\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294d161c-16c3-44dd-be81-ea25a07879b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small = df_small.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a01b4af-f7c6-4446-aaa5-467fc7b8d0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.utils.format_bytes(\n",
    "    df_small.memory_usage(deep=True).sum().compute()\n",
    ")\n",
    "#'10.55 GiB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7e563e-200f-4e53-aa55-58e06ca9ee49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small.base_passenger_fare.sum().compute() / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4667e110-979e-4782-9644-85f02e1ad2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small.driver_pay.sum().compute() / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834816b0-5899-43a8-853e-ffc91a0a7da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40bd244-96f5-4863-9625-325caf489cbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744bb47c-bf29-4274-9490-3c8155330798",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
